# MCTS with Neural Nets for Wallwars

## Overview

The aim of this project is to develop an AlphaGo-inspired MCTS for the abstract strategy game
[Wallwars](https://www.wallwars.net).
In particular, it is supposed to achieve three objectives:

1. Be competitive with humans on larger boards.
2. Beat the negamax-based AI that is already on the website on an 8x8 board.
3. Train a good model in under 24 hours on consumer hardware.

## Design

In order to achieve goal 3, the code is written in a way to optimize self-play performance unlike a
lot of toy Python implementations that are out there.

The ResNet model used for inference is compiled into an optimized form using TensorRT and self-play
is done in C++.
Inferences are cached using a sharded LRU cache which saves some work in the case of repeating or
early game positions.
However, the main challenge is that in order to fully utilize a modern GPU, we need to feed a
constant stream of decently sized batches (lets say at least 128 inferences per batch, the bigger
the better) to the GPU.

The idea for implementing this efficiently is that each sample down the tree in our MCTS is a
coroutine.
These coroutines can be scheduled on a thread pool so that multiple samples both from the same MCTS
instance and across different MCTS instances can be in-flight at the same time.
This allows us to efficiently keep 20+ CPU cores 100% used during training even as games are
starting and stopping.

Moreover, when a sample reaches a leaf node where it would need to get an output from our model, it
awaits on a `BatchedModel`.
Effectively, it puts its inference request on a lock-free multi-producer multi-consumer queue and
suspends itself.
One or more worker threads pull inference requests from the queue in batches, have them handled on
the GPU with TensorRT, and then resume the corresponding sample coroutines with their data.
As long as our CPU threads are generating enough samples, this provides a constant stream of equal
sized batches for the GPU.
Ideally at least two worker threads should be used to mitigate the time spent on preparing the input
and delivering the output.

## Workflow

A new model can be trained using `scripts/training.py` (by default it assumes you're executing it
from the `scripts` folder as your working directory).
It has some reasonable defaults and relatively self-explanatory command line arguments.
Fine-tuning the many parameters and the model architecture is work in progress.

It is possible to rank the models generated via training (or any set of models in a folder) with
the `--ranking` flag to `deep_ww`. This mode plays a series of random tournaments among the models
and stores all the games in a `json` file (for analysis) and a `pgn` file (for Elo generation).
The `BayesianElo` tool (see `externals/BayesianElo`) can be used to determine Elo values of the
models which can be plotted with `scripts/plot_elo.py`. An example for the `8x8_750000` model
is shown below, training took around 100 hours on an RTX 5080:

![Elo progression during training](assets/plots/elo_progression.png)

## Pre-Trained Models

There are some pre-trained models for testing in the `assets/models` folder. The naming convention
is `{columns}x{rows}_{games used for training}.{extension}`. Each model comes in three formats:

* `.pt` which can be loaded by PyTorch.
* `.onnx` which is a general purpose model format.
* `.trt` which is an optimized representation generated by TensorRT.

Since the `.trt` files are specific to the user hardware, they are not included and need to be
generated with `trtexec --onnx={path to onnx model} --saveEngine={output path} --fp16`.

## Dependencies (C++)

Required:
* folly (for coroutines, logging, thread pools, and more)
* CUDA & TensorRT (for inference)

Optional:
* Catch2 v3 (for unit tests)
* SFML (for GUI)

## Dependencies (Python)

* PyTorch (for model creation) w/ onnx (for exporting)
* fastai  (for training)

## GUI Font Setup

The GUI includes a bundled DejaVu Sans font (`assets/gui/fonts/DejaVuSans.ttf`) for reliable text
rendering across all platforms which gets bundled into the binary at build time, so no additional
configuration is needed.

## Contributions

Contributions are welcome, please make sure to setup the formatting pre-comit hook with the
`setup-hooks.sh` script in the repo.
